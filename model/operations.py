import torch


class MLP(torch.nn.Module):
    """"""
    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int, num_layers: int, act: str = "GELU"):
        """ Multi layer perceptron

        This class is the implementation of a multi layer perceptron with a linear layer and activations.
        The first layer is of size (in_channels, hidden_channels) followed by an activation function.
        The next `num_layers` are of size (hidden_channels,hidden_channels) followed by an activation function.
        The last layer is of size (hidden_channels, out_channels) without activation.

        Parameters
        ----------
        in_channels: int
            Size of the input channel.
        hidden_channels: int
            Size of all hidden channel.
        out_channels: int
            Output size.
        num_layers: int
            Number of hidden layers.
        act: str
            Name of the activation function to use (ReLU, GELU, Sigmoid...).
            See https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity for more
            activation function names.

        Example
        -------
        For a model taking an input of 10 features, 5 hidden layers of size 256 and an output size of 2:

            model = MLP(10,256,2,5)
            model(x)
            >> torch.Tensor(...)

        """
        super(MLP, self).__init__()
        print("test ")

        self.in_channels = int(in_channels)
        self.hidden_channels = int(hidden_channels)
        self.out_channels = int(out_channels)
        self.num_layers = int(num_layers)
        self.act = str(act)

        self.activation = getattr(torch.nn, self.act)

        layers = [torch.nn.Linear(self.in_channels, self.hidden_channels),
                  self.activation()]

        for _ in range(self.num_layers):
            layers.append(torch.nn.Linear(self.hidden_channels, self.hidden_channels))
            layers.append(self.activation())

        layers.append(torch.nn.Linear(self.hidden_channels, self.out_channels))

        self.model = torch.nn.Sequential(*layers)

    def forward(self, x):
        """Forward pass on the model.

        Compute the forward pass by applying the input data `x` to the neural network.

        Parameters
        ----------
        x: torch.Tensor
            Input data of shape [n, self.in_channels].

        Returns
        -------
        torch.Tensor:
            Output tensor of shape [n, self.out_channels] generated by the MLP.
        """
        return self.model(x)
