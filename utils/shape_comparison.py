import pandas as pd
import torch
import torch_geometric as tg
from tqdm import tqdm
from pathlib import Path
from sklearn.neighbors import KDTree
import numpy as np
import pyvista as pv
from copy import deepcopy
import time
from multiprocessing import Pool
from itertools import repeat
import multiprocessing as mp


def surface_reconstruction_error(folder: str, configuration: dict):
    """ Compute the deformation error on the mesh surfaces.

    Using the .pt results file of AMGNN generated by applying the best model to the test dataset this function is
    calculating the distance between the label deformed mesh and the predicted deformed mesh.

    Parameters
    ----------
    folder: str
        Path where are the saved .pt files.
    configuration: dict
        Configuration used to unnormalise the deformation and points position for the mesh reconstruction step.

    Returns
    -------
    pandas.DataFrame:
        DataFrame containing for each mesh file, the sum, max and mean reconstruction error
    """
    np.seterr(invalid="ignore")
    results = get_empty_results_df()

    # List all bacth file to process
    batchs = Path(folder).glob("*.pt")
    batchs = list(batchs)

    for batch_file in tqdm(batchs):
        batch = torch.load(batch_file)

        local_results = get_empty_results_df()
        local_results = list()
        graphs = [batch[i] for i in range(batch.num_graphs)]

        ctx = mp.get_context('spawn')
        # q = ctx.Queue()
        processes = []
        q = mp.Queue()
        q.put(local_results)
        for graph in tqdm(graphs):
            p = mp.Process(target=compute_error, args=(q,
                                                       graph,
                                                       folder,
                                                       configuration["scaling_deformation"], ))
            processes.append(p)
            p.start()

        #for p in tqdm(processes):
        #    local_result = q.get()
        #    local_results.append(local_result)

        for p in tqdm(processes):
            p.join()


    for vtk_file in Path(folder).glob("*_AMGNN_result.vtk"):
        local_results = get_empty_results_df()

    results = pd.concat([results, local_results], ignore_index=True, sort=False)

    return results


def compute_error(queue, graph, folder, scaling_deformation):
    AMGNN = graph_error(graph, scaling_deformation)
    # Compute the error values
    sum_error = np.nansum(AMGNN["Distances from simulation (mm)"])
    mean_error = np.nanmean(AMGNN["Distances from simulation (mm)"])

    # Save file deformed by AMGNN
    save_path = Path(folder) / f"{graph.file_name}_AMGNN_result.vtk"
    AMGNN.save(save_path)
    #local_results = queue.get()
    # local_results.loc[len(local_results)] = [graph.file_name, sum_error, mean_error]
    #local_results.append([graph.file_name, sum_error, mean_error])
    #queue.put(local_results)


def get_empty_results_df() -> pd.DataFrame:
    """ Generate an empty results dataframe.
    """
    results = pd.DataFrame(columns=["Name",  # Mesh file name
                                    "Sum_error",  # Sum of the norms of all error vectors,
                                    "Mean_error"])  # Mean of the norms of all error vectors,
    return results


def time_step(name, past_time):
    elapsed_time = time.perf_counter() - past_time
    print(f" {name} Elapsed time: {elapsed_time:0.4f} seconds")
    return time.perf_counter()


def graph_error(graph, scaling_deformation):
    graph.pos = graph.pos * 1000  # Transform the position from meter to millimeter
    graph.y, graph.y_hat = torch.split(graph.y, 4, dim=1)

    # normal vector of all points
    normal_v = torch.zeros_like(graph.pos)
    deformation_label = graph.y[:, 1:]
    deformation_amgnn = graph.y_hat[:, 1:]

    position = torch.round(graph.pos, decimals=0)  # rounded millimeters position

    unique_position, index_pos, counts = torch.unique(position, dim=0, return_inverse=True, sorted=False,
                                                      return_counts=True)
    unique_deformation_label = torch.zeros_like(unique_position)
    unique_deformation_amgnn = torch.zeros_like(unique_position)
    unique_normal = torch.zeros_like(unique_position)

    # For all unique position, where are creating a deformation label vector
    for u_p, i in zip(unique_position, range(len(counts))):
        # Gather all position index where of this unique position
        duplicate_indice = (position == u_p).all(axis=1).nonzero(as_tuple=True)[0]

        # Make the mean of those deformation
        unique_deformation_label[i] = torch.mean(deformation_label[duplicate_indice], axis=0)
        unique_deformation_amgnn[i] = torch.mean(deformation_amgnn[duplicate_indice], axis=0)

    deformation_label = unique_deformation_label
    deformation_amgnn = unique_deformation_amgnn
    position = unique_position

    # New normal vector
    # normal vector of all points
    normal_v = torch.zeros_like(position)

    max_z = torch.max(position[:, 2])
    min_z = torch.min(position[:, 2])

    is_top = torch.isclose(position[:, 2], max_z)
    is_bottom = torch.isclose(position[:, 2], min_z)
    top_layer = position[is_top]
    bottom_layer = position[is_bottom]

    # On the top layer, all node are evenly spaced. A normal vector [0,0,1] will be given to all of those points
    normal_v[is_top] = torch.Tensor([0, 0, 1])
    normal_v[is_bottom] = torch.Tensor([0, 0, -1])

    all_z = torch.unique(position[:, 2])[1:][:-1]  # All Z exept the first and last layers

    # Process all layers
    for layer_z in all_z:
        is_layer = torch.isclose(position[:, 2], layer_z)
        selected_layer = position[is_layer]
        tree = KDTree(selected_layer.numpy())

        skin = np.zeros(len(selected_layer))
        # normal_v
        layer_normal = np.zeros_like(selected_layer)

        for point_id in range(len(selected_layer)):
            neighbors = tree.query_radius([selected_layer[point_id].numpy()], r=7, count_only=False)
            numb_neighbors = len(neighbors[0]) - 1  # remove self

            # If there is less than 4 neighbors then the points is on the skin of the part.
            if numb_neighbors < 4:
                skin[point_id] = 1

                # Normal of the points
                neighbors = np.delete(neighbors[0], neighbors[0] == point_id)
                vector = selected_layer[point_id].numpy() - np.mean(selected_layer[neighbors].numpy(), axis=0)
                vector = vector / np.linalg.norm(vector)
                layer_normal[point_id] = vector

        if not skin.any():  # If no skin was registred, there is a problem
            print(f"Pb layers {layer_z}")
            raise "PB"

        normal_v[is_layer] = torch.Tensor(layer_normal)
    # Surface reconstruction

    mask_usefull_points = np.isclose(normal_v.numpy(), np.asarray([0., 0., 0.])).all(axis=1)
    mask_usefull_points = np.invert(mask_usefull_points)

    points = pv.wrap(position.numpy()[mask_usefull_points])

    surf = points.delaunay_3d()
    # For all unique position, where are creating a deformation label vector

    for u_p, i in zip(unique_position, range(len(counts))):
        # Gather all position index where of this unique position
        duplicate_indice = (position == u_p).all(axis=1).nonzero(as_tuple=True)[0]

        unique_normal[i] = torch.nanmean(normal_v[duplicate_indice], axis=0)

    # Add deformation to the meshes

    deformation = pv.PolyData(position.numpy())
    deformation["Simufact"] = 2 * scaling_deformation * deformation_label.numpy() - scaling_deformation
    deformation["AMGNN"] = 2 * scaling_deformation * deformation_amgnn.numpy() - scaling_deformation
    deformation["Normal"] = unique_normal.numpy()

    interpolated = surf.interpolate(deformation, radius=12.0)
    AMGNN = deepcopy(interpolated)
    Simu = deepcopy(interpolated)

    AMGNN.points += AMGNN["AMGNN"]
    Simu.points += Simu["Simufact"]
    AMGNN["Difference"] = AMGNN["AMGNN"] - AMGNN["Simufact"]
    # Compute error
    AMGNN["Distances from simulation (mm)"] = np.empty(AMGNN.n_points)

    p = AMGNN.points
    vec = AMGNN["Normal"] * 2
    p0 = p - vec
    p1 = p + vec
    for i in range(AMGNN.n_points):
        ip, ic = Simu.extract_geometry().ray_trace(p0[i], p1[i], first_point=True)
        dist = np.sqrt(np.sum((ip - p[i]) ** 2))
        AMGNN["Distances from simulation (mm)"][i] = dist

    return AMGNN


def init_worker(AMGNN):
    global shared_AMGNN

    shared_AMGNN = AMGNN


def compute_distance_from_simu(Simu, i):
    # p = AMGNN.points[i]
    # vec = AMGNN["Normal"][i]  # * Simu_n.length
    # p0 = p - vec * 2  # * 10
    ##p1 = p + vec * 2  # * 10
    ip, ic = Simu.extract_geometry().ray_trace(shared_AMGNN["p0"][i], shared_AMGNN["p1"][i], first_point=True)
    dist = np.sqrt(np.sum((ip - shared_AMGNN["p"][i]) ** 2))
    shared_AMGNN["Distances from simulation (mm)"][i] = dist


if __name__ == "__main__":
    alignment_error = surface_reconstruction_error(
        folder=r"E:\Leopold\Chapter 6 - datasets\Cubes\simple_conv_[1, 0, 0] test_output",
        configuration={"scaling_deformation": 0.2})
    mean_alignment_error = alignment_error["Mean_error"].mean()
    max_alignment_error = alignment_error["Mean_error"].max()
    min_alignment_error = alignment_error["Mean_error"].min()

    print(mean_alignment_error, max_alignment_error, min_alignment_error)
