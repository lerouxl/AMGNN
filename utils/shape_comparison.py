import pandas as pd
import torch
from tqdm import tqdm
from pathlib import Path
from sklearn.neighbors import KDTree
import numpy as np
import pyvista as pv
from copy import deepcopy
import time
import multiprocessing as mp
from itertools import repeat

# Flag to select if the error processing is done by multithreading
# On some configuration, the multithreading is not working well with Pytorch. You can turn it off.
USE_MULTITHREADING = True
def surface_reconstruction_error(folder: str, configuration: dict):
    """ Compute the deformation error on the mesh surfaces.

    Using the .pt results file of AMGNN generated by applying the best model to the test dataset this function is
    calculating the distance between the label deformed mesh and the predicted deformed mesh.

    Parameters
    ----------
    folder: str
        Path where are the saved .pt files.
    configuration: dict
        Configuration used to unnormalise the deformation and points position for the mesh reconstruction step.

    Returns
    -------
    pandas.DataFrame:
        DataFrame containing for each mesh file, the sum, max and mean reconstruction error
    """
    global USE_MULTITHREADING
    np.seterr(invalid="ignore")
    results = get_empty_results_df()

    # List all bacth file to process
    batchs = Path(folder).glob("*.pt")
    batchs = list(batchs)

    # Do a multi processing a all files in batchs.
    for batch_file in batchs:
        # load batch
        batch = torch.load(batch_file)

        if hasattr(batch, "num_graphs"):
            graphs = [batch[i] for i in range(batch.num_graphs)]
        else:
            graphs = [batch]
            USE_MULTITHREADING = False

        # On some configuration, the multithreading is not working well with Pytorch. You can deactivate it.
        if USE_MULTITHREADING:
            pool = mp.Pool(5)
            pool.starmap(compute_error, zip(graphs, repeat(folder), repeat(configuration["scaling_deformation"])))
        else:
            print("SLOW PROCESS MODE USED")
            for graph in graphs:
                compute_error(graph, folder, configuration["scaling_deformation"])

    # Read all vtk output file and generate the metrics
    for vtk_file in Path(folder).glob("*_AMGNN_result.vtk"):
        csv_line = [] # Data to add to the csv

        mesh = pv.read(vtk_file)
        csv_line.append(vtk_file.name)

        # Distances from simulation
        array_ = np.array(mesh["Distances from simulation (mm)"])
        dist_sum_error = np.nansum(array_)
        dist_mean_error = np.nanmean(array_)
        dist_median_error = np.nanmedian(array_)
        dist_max_error = np.nanmax(array_)
        dist_std_error = np.nanstd(array_)
        csv_line.extend([dist_sum_error, dist_mean_error, dist_median_error, dist_max_error, dist_std_error])

        # MSE
        array_ = mesh["MSE"]
        array_ = np.array(array_)
        mse_sum_error = np.nansum(array_)
        mse_mean_error = np.nanmean(array_)
        mse_median_error = np.nanmedian(array_)
        mse_max_error = np.nanmax(array_)
        mse_std_error = np.nanstd(array_)
        csv_line.extend([mse_sum_error,mse_mean_error,mse_median_error,mse_max_error,mse_std_error])

        # L1
        array_ = mesh["L1"]
        array_ = np.array(array_)
        l1_sum_error = np.nansum(array_)
        l1_mean_error = np.nanmean(array_)
        l1_median_error = np.nanmedian(array_)
        l1_max_error = np.nanmax(array_)
        l1_std_error = np.nanstd(array_)
        csv_line.extend([l1_sum_error,l1_mean_error,l1_median_error,l1_max_error,l1_std_error])

        # L2
        array_ = mesh["L2"]
        array_ = np.array(array_)
        l2_sum_error = np.nansum(array_)
        l2_mean_error = np.nanmean(array_)
        l2_median_error = np.nanmedian(array_)
        l2_max_error = np.nanmax(array_)
        l2_std_error = np.nanstd(array_)
        csv_line.extend([l2_sum_error, l2_mean_error, l2_median_error, l2_max_error,l2_std_error])

        # Closest surface error
        array_ = mesh["Closest surface error (mm)"]
        array_ = np.array(array_)
        closest_surface_sum_error = np.nansum(array_)
        closest_surface_mean_error = np.nanmean(array_)
        closest_surface_median_error = np.nanmedian(array_)
        closest_surface_max_error = np.nanmax(array_)
        closest_surface_std_error = np.nanstd(array_)
        csv_line.extend([closest_surface_sum_error, closest_surface_mean_error, closest_surface_median_error, closest_surface_max_error,closest_surface_std_error])

        nb_points = len(mesh.points)
        csv_line.append(nb_points)

        results.loc[len(results)] = csv_line

    # Compute the error statistic on all
    l1_mean_total_error = (results["L1_mean_error"] * results["Nb_points"]).sum() / results["Nb_points"].sum()
    l2_mean_total_error = (results["L2_mean_error"] * results["Nb_points"]).sum() / results["Nb_points"].sum()
    mse_mean_total_error = (results["MSE_mean_error"] * results["Nb_points"]).sum() / results["Nb_points"].sum()
    dist_mean_total_error = (results["Dist_mean_error"] * results["Nb_points"]).sum() / results["Nb_points"].sum()
    closest_surface_total_mean = (results["Closest_surface_mean"] * results["Nb_points"]).sum() / results["Nb_points"].sum()

    l1_std_total_error = (results["L1_std_error"] * results["Nb_points"]).sum() / results["Nb_points"].sum()
    l2_std_total_error = (results["L2_std_error"] * results["Nb_points"]).sum() / results["Nb_points"].sum()
    mse_std_total_error = (results["MSE_std_error"] * results["Nb_points"]).sum() / results["Nb_points"].sum()
    dist_std_total_error = (results["Dist_std_error"] * results["Nb_points"]).sum() / results["Nb_points"].sum()
    closest_surface_total_std = (results["Closest_surface_std"] * results["Nb_points"]).sum() / results["Nb_points"].sum()

    results.loc[len(results)] = ["total",
                                 "Dist_sum_error",
                                 dist_mean_total_error,
                                 "Dist_median_error",
                                 "Dist_max_error",
                                 dist_std_total_error,
                                 "MSE_sum_error",
                                 mse_mean_total_error,
                                 "MSE_median_error",
                                 "MSE_max_error",
                                 mse_std_total_error,
                                 "L1_sum_error",
                                 l1_mean_total_error,
                                 "L1_median_error",
                                 "L1_max_error",
                                 l1_std_total_error,
                                 "L2_sum_error",
                                 l2_mean_total_error,
                                 "L2_median_error",
                                 "L2_max_error",
                                 l2_std_total_error,
                                 closest_surface_sum_error,
                                 closest_surface_total_mean,
                                 closest_surface_median_error,
                                 closest_surface_max_error,
                                 closest_surface_total_std,
                                 results["Nb_points"].sum()]

    return results


def compute_error(graph, folder, scaling_deformation):
    AMGNN = graph_error(graph, scaling_deformation)

    # Save file deformed by AMGNN
    save_path = Path(folder) / f"{graph.file_name}_AMGNN_result.vtk"
    AMGNN.save(save_path)

def get_empty_results_df() -> pd.DataFrame:
    """ Generate an empty results dataframe.
    """
    results = pd.DataFrame(columns=["Name",  # Mesh file name
                                    "Dist_sum_error",
                                    "Dist_mean_error",
                                    "Dist_median_error",
                                    "Dist_max_error",
                                    "Dist_std_error",
                                    "MSE_sum_error",
                                    "MSE_mean_error",
                                    "MSE_median_error",
                                    "MSE_max_error",
                                    "MSE_std_error",
                                    "L1_sum_error",
                                    "L1_mean_error",
                                    "L1_median_error",
                                    "L1_max_error",
                                    "L1_std_error",
                                    "L2_sum_error",
                                    "L2_mean_error",
                                    "L2_median_error",
                                    "L2_max_error",
                                    "L2_std_error",
                                    "Closest_surface_sum",
                                    "Closest_surface_mean",
                                    "Closest_surface_median",
                                    "Closest_surface_max",
                                    "Closest_surface_std",
                                    "Nb_points"])  # Mean of the norms of all error vectors,
    return results


def time_step(name, past_time):
    elapsed_time = time.perf_counter() - past_time
    print(f" {name} Elapsed time: {elapsed_time:0.4f} seconds")
    return time.perf_counter()


def graph_error(graph, scaling_deformation, m_to_mm=True):
    """
    From a graph of points and deformation, recreate the surface and compute the difference between the prediction and label.

    Parameters
    ----------
    graph: tg.data.Data with pos [n,3] and y [n,8]
    scaling_deformation: Scale used to unnormalise the deformation
    m_to_mm: bool if the pos data must be scaled by 1000

    Returns
    -------

    """
    if m_to_mm:
        graph.pos = graph.pos * 1000  # Transform the position from meter to millimeter
    graph.y, graph.y_hat = torch.split(graph.y, 4, dim=1)

    # normal vector of all points
    normal_v = torch.zeros_like(graph.pos)
    deformation_label = graph.y[:, 1:]
    deformation_amgnn = graph.y_hat[:, 1:]

    position = torch.round(graph.pos, decimals=0)  # rounded millimeters position

    unique_position, index_pos, counts = torch.unique(position, dim=0, return_inverse=True, sorted=False,
                                                      return_counts=True)
    unique_deformation_label = torch.zeros_like(unique_position)
    unique_deformation_amgnn = torch.zeros_like(unique_position)
    unique_normal = torch.zeros_like(unique_position)

    # For all unique position, where are creating a deformation label vector
    for u_p, i in zip(unique_position, range(len(counts))):
        # Gather all position index where of this unique position
        duplicate_indice = (position == u_p).all(axis=1).nonzero(as_tuple=True)[0]

        # Make the mean of those deformation
        unique_deformation_label[i] = torch.mean(deformation_label[duplicate_indice], axis=0)
        unique_deformation_amgnn[i] = torch.mean(deformation_amgnn[duplicate_indice], axis=0)

    deformation_label = unique_deformation_label
    deformation_amgnn = unique_deformation_amgnn
    position = unique_position

    # New normal vector
    # normal vector of all points
    normal_v = torch.zeros_like(position)

    max_z = torch.max(position[:, 2])
    min_z = torch.min(position[:, 2])

    is_top = torch.isclose(position[:, 2], max_z)
    is_bottom = torch.isclose(position[:, 2], min_z)
    top_layer = position[is_top]
    bottom_layer = position[is_bottom]

    # On the top layer, all node are evenly spaced. A normal vector [0,0,1] will be given to all of those points
    normal_v[is_top] = torch.Tensor([0, 0, 1])
    normal_v[is_bottom] = torch.Tensor([0, 0, -1])

    all_z = torch.unique(position[:, 2])[1:][:-1]  # All Z exept the first and last layers

    # Process all layers
    for layer_z in all_z:
        is_layer = torch.isclose(position[:, 2], layer_z)
        selected_layer = position[is_layer]
        tree = KDTree(selected_layer.numpy())

        skin = np.zeros(len(selected_layer))
        # normal_v
        layer_normal = np.zeros_like(selected_layer)

        for point_id in range(len(selected_layer)):
            neighbors = tree.query_radius([selected_layer[point_id].numpy()], r=7, count_only=False)
            numb_neighbors = len(neighbors[0]) - 1  # remove self

            # If there is less than 4 neighbors then the points is on the skin of the part.
            if numb_neighbors < 4:
                skin[point_id] = 1

                # Normal of the points
                neighbors = np.delete(neighbors[0], neighbors[0] == point_id)
                vector = selected_layer[point_id].numpy() - np.mean(selected_layer[neighbors].numpy(), axis=0)
                vector = vector / np.linalg.norm(vector)
                layer_normal[point_id] = vector

        if not skin.any():  # If no skin was registred, there is a problem
            print(f"Pb layers {layer_z}")
            raise "PB"

        normal_v[is_layer] = torch.Tensor(layer_normal)
    # Surface reconstruction

    mask_usefull_points = np.isclose(normal_v.numpy(), np.asarray([0., 0., 0.])).all(axis=1)
    mask_usefull_points = np.invert(mask_usefull_points)

    points = pv.wrap(position.numpy()[mask_usefull_points])

    surf = points.delaunay_3d()
    # For all unique position, where are creating a deformation label vector

    for u_p, i in zip(unique_position, range(len(counts))):
        # Gather all position index where of this unique position
        duplicate_indice = (position == u_p).all(axis=1).nonzero(as_tuple=True)[0]

        unique_normal[i] = torch.nanmean(normal_v[duplicate_indice], axis=0)

    # Add deformation to the meshes

    deformation = pv.PolyData(position.numpy())
    deformation["Simufact"] = 2 * scaling_deformation * deformation_label.numpy() - scaling_deformation
    deformation["AMGNN"] = 2 * scaling_deformation * deformation_amgnn.numpy() - scaling_deformation
    deformation["Normal"] = unique_normal.numpy()

    interpolated = surf.interpolate(deformation, radius=4.5)
    AMGNN = deepcopy(interpolated)
    Simu = deepcopy(interpolated)

    AMGNN.points += AMGNN["AMGNN"]
    Simu.points += Simu["Simufact"]
    AMGNN["Difference"] = AMGNN["AMGNN"] - AMGNN["Simufact"]
    AMGNN["MSE"] = np.sqrt((AMGNN["Difference"])**2).mean(axis=1)
    AMGNN["L1"] = np.abs(AMGNN["Difference"]).sum(axis=1)
    AMGNN["L2"] = np.sqrt(((AMGNN["Difference"]) ** 2).sum(axis=1))
    # Compute error
    # How this is calculated https://github.com/pyvista/pyvista/discussions/1834
    AMGNN.compute_implicit_distance(Simu.extract_surface(), inplace=True)
    AMGNN["Distances from simulation (mm)"] = np.abs(AMGNN['implicit_distance'])

    AMGNN["Closest surface error (mm)"] = np.empty(AMGNN.n_points)
    p = AMGNN.points
    p_simufact = Simu.points
    vec = AMGNN["Normal"] * 2 * scaling_deformation
    p0 = p - vec
    p1 = p + vec
    for i in range(AMGNN.n_points):
        #ip, ic = Simu.extract_geometry().ray_trace(p0[i], p1[i], first_point=True)
        #dist = np.sqrt(np.sum((ip - p[i]) ** 2))
        closest_cells, closest_points = AMGNN.find_closest_cell(p_simufact[i], return_closest_point=True)
        dist = np.linalg.norm(p_simufact[i] - closest_points)
        AMGNN["Closest surface error (mm)"][i] = dist

    return AMGNN


if __name__ == "__main__":
    from pathlib import Path

    super_folder = Path(r"E:\Leopold\Chapitre 6 - AMGNN\Results\Sweep 1 HilbertCube1 shape\test_output")
    folders = [x for x in super_folder.glob("*test_output") if x.is_dir()]
    print(f"{len(folders)} folders detected")

    for i,folder_path in enumerate(folders):
        #try:
            # Choose the folder to analyse
            #folder_path = Path(r"E:\Leopold\Chapter 6 - datasets\Cubes\2022_12_28_10_29_simple_mlp_0.7_0.15_0.15 test_output")

            # Analyse it
        alignment_error = surface_reconstruction_error(
            folder=folder_path,
            configuration={"scaling_deformation": 0.2})

        # Save the csv results
        alignment_error.to_csv(folder_path / "alignment_error.csv")
        alignment_error.to_csv(super_folder / f"{folder_path.name}.csv")

        df = alignment_error.iloc[-1:]
        print(f'{i+1} - The {folder_path.name} L2 mean error is {float(df["L2_mean_error"])} mm')
        try:
            pass
        except Exception as e:
            print(f'{i+1} - FAIL {folder_path.name} : {e}')
